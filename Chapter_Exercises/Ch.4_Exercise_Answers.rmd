---
title: "Ch.4_Exercise_Answers"
author: "Patrick Edwards"
date: '2022-06-13'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 4 Exercise Answers.

## 4E1. In the model definition below, which line is the likelihood?
$$
y_i ~ \sim \text{Normal}(\mu, \sigma) \\
\mu \sim \text{Normal}(0,10) \\
\sigma \sim \text{Exponential}(1)
$$

Line 1 is the likelihood. It says that the probability of the observed outcome is normally distributed with mean $\mu$ and standard deviation $\sigma$. The other lines represent the prior distributions for the two parameters $\mu$ and $\sigma$.


## 4E2. In the model definition just above, how many parameters are in the posterior distribution?

The posterior distribution has two parameters: $\mu$ and $\sigma$.


## 4E3. Using the model definition above, write down the appropriate form of Bayes’ theorem that includes the proper likelihood and priors.

Note that there's more than one observation in $y$, so to get the joint likelihood across all the data, we have to compute the probability for each $y_i$ and then multiply all these likelihoods together.
$$
\text{Pr}(\mu, \sigma | y) = \frac{\prod \text{Normal}(y | \mu, \sigma) \cdot \text{Normal}(\mu | 0,10) \cdot \text{Exponential}(\sigma | 1)}{\int_{\mu} \int_{\sigma} \prod \text{Normal}(y | \mu, \sigma) \cdot \text{Normal}(\mu | 0,10) \cdot \text{Exponential}(\sigma | 1)  d\sigma d\mu}
$$


## 4E4. In the model definition below, which line is the linear model?
$$
y_i ~ \sim \text{Normal}(\mu, \sigma) \\
\mu_i = \alpha + \beta x_i \\
\alpha \sim \text{Normal}(0, 10) \\
\beta \sim \text{Normal}(0,1) \\
\sigma \sim \text{Exponential(2)}
$$

Line 2 contains the linear model.


## 4E5. In the model definition just above, how many parameters are in the posterior distribution?

There are three parameters in the posterior distribution: $\alpha$, $\sigma$, and $\beta$. Note that $\mu_i$ is a combination of other parameters and observed data.


## 4M1. For the model definition below, simulate observed y values from the prior (not the posterior).
$$
y_i ~ \sim \text{Normal}(\mu, \sigma) \\
\mu_i \sim \text{Normal}(0, 10) \\
\sigma \sim \text{Exponential(1)}
$$

First, create a `quap` object.
```{r}
library(tidyverse)
library(dplyr)
library(rethinking)
library(ggplot2)

# Simulate values using tibble object
simulation <- tibble(
  mu = rnorm(
    n = 10000,
    mean = 0, 
    sd = 10
  ),
  sigma = rexp(
    n = 10000,
    rate = 1
  )
) %>%
  mutate(
    y = rnorm(
      n = 10000,
      mean = mu,
      sd = sigma
    )
  )

# Plot distribution:
ggplot2::ggplot(
  data = simulation,
  aes(x = y)) +
  geom_density() +
  labs(
    x = "y",
    y = "density"
  )

```



## 4M2. Translate the model just above into a `quap` formula.

Create `quap` list:
```{r}
func_list <- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dexp(1)
)
```


## 4M3. Translate the `quap` model formula below into a mathematical model definition.
```{r eval = FALSE}
y ~ dnorm(mu, sigma)

mu <- a + b*x

a ~ dnorm(0, 10)

b ~ dunif(0, 1)

sigma ~ dexp(1)
```

$$
y_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = a + b \cdot x_i \\
a \sim \text{Normal}(0, 10) \\
b \sim \text{Uniform}(0, 1) \\
\sigma \sim \text{Expontential}(1)
$$


## 4M4. A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

For student $i$ in year $t$:

**Likelihood function**: student $i$'s height is normally distributed around student $i$'s predicted height in year $t$, $\mu_{it}$, with standard deviation $\sigma$:
$$
h_i \sim \text{Normal}(\mu_{it}, \sigma)
$$

**Linear Model**: student $i$'s predicted height in year $t$, $\mu_{it}$, is centered on the average year $t = 2$. These terms are a linear combination of intercept parameter $\alpha$ and slope parameter $\beta$:
$$
\mu_{it} = \alpha + \beta \cdot (y_t - \overline{y})
$$

Priors:

**$\alpha$ prior**: measured in centimeters with still-growing children. Average height for mean year $t = 2$ is 100 centimeters with standard deviation 10 centimeters.
$$
\alpha \sim \text{Normal}(100, 10)
$$

**$\beta$ prior**: This prior doesn't assume that children grow, so the prior allows children to grow and even shrink. The prior is normally distributed and centered on zero with standard deviation of 10 centimeters.
$$
\beta \sim \text{Normal}(0, 10)
$$

**$\sigma$ prior**: this exponential prior assumes that the average standard deviation is 1.
$$
\sigma \sim \text{Exponential}(1)
$$

Let's graph it the lines for 50 simulated children.
```{r}
n <- 50 # 50 simulated children.

tibble(
  group = seq_len(n),
  alpha = rnorm(
    n,
    mean = 100,
    sd = 10
  ),
  beta = rnorm(
    n,
    mean = 0,
    sd = 10
  ),
  sigma = rexp(
    n,
    rate = 1
  )
) %>%
  expand(
    nesting(group, alpha, beta, sigma),
    year = c(1, 2, 3)
  ) %>%
  mutate(
    height = rnorm(
      n(),
      alpha + beta * (year - mean(year)),
      sigma
    )
  ) %>%
  ggplot(
    aes(
      x = year,
      y = height,
      group = group 
    )
  ) +
  geom_line() +
  labs(
    x = "Year",
    y = "Height"
  )
```


## 4M5. Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?

If we know that the children's height is strictly increasing each year, then we can restrict the slope parameter $\beta$ to be greater than zero. From the chapter, we know that the easiest way to do this is to define the prior for $\beta$ as Log-Normal:
$$
\beta \sim \text{Log-Normal}(1, 0.5)
$$

This means using the `rlnorm` function in R. Let's see how this changes the plot:
```{r}
n <- 50 # 50 simulated children.

tibble(
  group = seq_len(n),
  alpha = rnorm(
    n,
    mean = 100,
    sd = 10
  ),
  beta = rlnorm(
    n,
    mean = 1,
    sd = 0.5
  ),
  sigma = rexp(
    n,
    rate = 1
  )
) %>%
  expand(
    nesting(group, alpha, beta, sigma),
    year = c(1, 2, 3)
  ) %>%
  mutate(
    height = rnorm(
      n(),
      alpha + beta * (year - mean(year)),
      sigma
    )
  ) %>%
  ggplot(
    aes(
      x = year,
      y = height,
      group = group 
    )
  ) +
  geom_line() +
  labs(
    x = "Year",
    y = "Height"
  )
```


## 4M6. Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?

This means that, for a given year, the sample of students do not have a variance greater than 64 centimeters, or a standard deviation of $\sqrt{64} = \sqrt{4 \cdot 4 \cdot 4} = 2 \cdot 2 \cdot 2 = 8$. 

Let's update our priors:

```{r}
n <- 50 # 50 simulated children.

tibble(
  group = seq_len(n),
  alpha = rnorm(
    n,
    mean = 100,
    sd = 10
  ),
  beta = rlnorm(
    n,
    mean = 1,
    sd = 0.5
  ),
  sigma = runif(
    n,
    min = 0,
    max = 8
  )
) %>%
  expand(
    nesting(group, alpha, beta, sigma),
    year = c(1, 2, 3)
  ) %>%
  mutate(
    height = rnorm(
      n(),
      alpha + beta * (year - mean(year)),
      sigma
    )
  ) %>%
  ggplot(
    aes(
      x = year,
      y = height,
      group = group 
    )
  ) +
  geom_line() +
  labs(
    x = "Year",
    y = "Height"
  )
```


## 4M7. Refit model `m4.3` from the chapter, but omit the mean weight `xbar` this time. Compare the new model’s posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models.

Load data from chapter, define `xbar`:
```{r}
library(rethinking)


data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18,]

# define the average weight, x-bar


xbar <- mean(d2$weight)
```

Model from chapter with and without `xbar`:
```{r}
m4.3_orig <- quap(
  alist(
    height ~ dnorm( mu, sigma ),

    mu <- a + b*( weight - xbar ),

    a ~ dnorm( 178, 20 ),

    b ~ dlnorm( 0, 1 ),

    sigma ~ dunif( 0, 50 )
  ), 
  data = d2 
)

m4.3_new <- quap(
  alist(
    height ~ dnorm( mu, sigma ),

    mu <- a + b*( weight ),

    a ~ dnorm( 178, 20 ),

    b ~ dlnorm( 0, 1 ),

    sigma ~ dunif( 0, 50 )
  ), 
  data = d2 
)

# Overall results comparison:
precis(m4.3_orig)
precis(m4.3_new)

# Covariance results comparison:
round(vcov(m4.3_orig), 3)
round(vcov(m4.3_new), 3)

pairs(m4.3_orig)
pairs(m4.3_new)
```

The original model's parameters had little to no covariance. The new model, however, has high covariance between the $\alpha$ and $\beta$ parameters. The old model also had a greater mean $\alpha$ estimate with a much smaller standard deviation than the new model

Use `gganimate` to show uncertainty in posterior distribution:
```{r}
library(rethinking)
library(brms)

data(Howell1)
how_dat <- Howell1 %>%
  filter(age >= 18) %>%
  mutate(weight_c = weight - mean(weight))

# first, duplicate model with `quap`
m4.3 <- quap(alist(height ~ dnorm(mu, sigma),
                   mu <- a + b * (weight_c),
                   a ~ dnorm(178, 20),
                   b ~ dlnorm(0, 1),
                   sigma ~ dunif(0, 50)),
             data = how_dat)

round(vcov(m4.3), 3)
#>           a     b sigma
#> a     0.073 0.000 0.000
#> b     0.000 0.002 0.000
#> sigma 0.000 0.000 0.037

# and then with brms
b4.3 <- brm(
  height ~ 1 + weight_c, 
  data = how_dat, 
  family = gaussian,
  prior = c(
    prior(
      normal(178, 20), 
      class = Intercept
    ),
    prior(
      lognormal(0, 1), 
      class = b, 
      lb = 0
    ),
    prior(
      uniform(0, 50), 
      class = sigma
    )
  ),
  iter = 28000, 
  warmup = 27000, 
  chains = 4, 
  cores = 4, 
  seed = 1234
)
#> Warning: It appears as if you have specified an upper bounded prior on a parameter that has no natural upper bound.
#> If this is really what you want, please specify argument 'ub' of 'set_prior' appropriately.
#> Warning occurred for prior 
#> sigma ~ uniform(0, 50)

as_draws_df(b4.3) %>%
  as_tibble() %>% 
  select(b_Intercept, b_weight_c, sigma) %>%
  cov() %>%
  round(digits = 3)
#>             b_Intercept b_weight_c sigma
#> b_Intercept       0.074      0.000 0.000
#> b_weight_c        0.000      0.002 0.000
#> sigma             0.000      0.000 0.038
```

```{r}
b4.3_nc <- brm(height ~ 1 + weight, data = how_dat, family = gaussian,
               prior = c(prior(normal(178, 20), class = Intercept),
                         prior(lognormal(0, 1), class = b, lb = 0),
                         prior(uniform(0, 50), class = sigma)),
               iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 1234)
#> Warning: It appears as if you have specified an upper bounded prior on a parameter that has no natural upper bound.
#> If this is really what you want, please specify argument 'ub' of 'set_prior' appropriately.
#> Warning occurred for prior 
#> sigma ~ uniform(0, 50)

as_draws_df(b4.3_nc) %>%
  as_tibble() %>% 
  select(b_Intercept, b_weight, sigma) %>%
  cov() %>%
  round(digits = 3)
#>             b_Intercept b_weight sigma
#> b_Intercept       3.653   -0.079 0.010
#> b_weight         -0.079    0.002 0.000
#> sigma             0.010    0.000 0.035
```

```{r}
library(gganimate)

weight_seq <- tibble(weight = seq(25, 70, length.out = 100)) %>%
  mutate(weight_c = weight - mean(how_dat$weight))

predictions <- bind_rows(
  predict(b4.3, newdata = weight_seq) %>%
    as_tibble() %>%
    bind_cols(weight_seq) %>%
    mutate(type = "Centered"),
  predict(b4.3_nc, newdata = weight_seq) %>%
    as_tibble() %>%
    bind_cols(weight_seq) %>%
    mutate(type = "Non-centered")
)

fits <- bind_rows(
  weight_seq %>%
    add_epred_draws(b4.3) %>%
    mutate(type = "Centered"),
  weight_seq %>%
    add_epred_draws(b4.3_nc) %>%
    mutate(type = "Non-centered")
) %>%
  ungroup()

bands <- fits %>%
  group_by(type, weight) %>%
  median_qi(.epred, .width = c(.67, .89, .97))

lines <- fits %>%
  filter(.draw %in% sample(unique(.data$.draw), size = 50))

ggplot(lines, aes(x = weight)) +
  facet_wrap(~type, nrow = 1) +
  geom_ribbon(data = predictions, aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3) +
  geom_lineribbon(data = bands, aes(y = .epred, ymin = .lower, ymax = .upper),
                  color = NA) +
  scale_fill_brewer(palette = "Blues", breaks = c(.67, .89, .97)) +
  geom_line(aes(y = .epred, group = .draw)) +
  geom_point(data = how_dat, aes(y = height), shape = 1, alpha = 0.7) +
  labs(x = "Weight", y = "Height", fill = "Interval") +
  transition_states(.draw, 0, 1)
```




## 4M8. In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of knots and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights—change the standard deviation of the prior and watch what happens. What do you think the combination of knot number and the prior on the weights controls?






